{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Lab M04 (Version 3)\n",
    "## Data Wrangling and Exploratory Data Analysis\n",
    "\n",
    "Practice Lab M04 will focus on how to do the data wrangling and exploratory data analysis on big data technologies such as pyspark\n",
    "\n",
    "## To do\n",
    "\n",
    "- Use pyspark to perform data wrangling\n",
    "- Conduct the exploratory data analysis with visulization.\n",
    "\n",
    "\n",
    "## Tasks 1 Data Wrangling on Pyspark\n",
    "### Task 1.1 Reading parquet file\n",
    "We first read the given data source **magic.parquet**. However, it is not in a standard readable format. The parquet file is in columnar storage format for efficient storage purpose (very common in cloud service scenario). Our goal in this task is to read this parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8cZF4LYMv6P"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Comment this if the data visualisations doesn't work on your side\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z5LqooDxesbH",
    "outputId": "f9e04635-3d99-423c-c4bd-44c4fe9da697"
   },
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgFV0UZXgvrO"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ob00ZyregwxW",
    "outputId": "29c041e7-dc7a-4057-baef-d7673662bd83"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
    "# input your code for - read the parquet file by using pyspark \n",
    "# print the pyspark dataframe schema\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Pyspark operation\n",
    "After having the dataframe for the given data in pyspark, let's do some wrangling operation by using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYxeARNakIZf",
    "outputId": "e3493107-1af6-49fb-e984-ec67288016a0"
   },
   "outputs": [],
   "source": [
    "# filter the data on column fWidth where fWidth > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcjSb5NbmeYu",
    "outputId": "6d83d912-d21f-4682-92a8-cc9f7ab9f9e9"
   },
   "outputs": [],
   "source": [
    "# filter the data on column fWidth where fWidth > 50 and fWidth < 70, then print the statistic information of 'fSize' (using describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s70I5mDQmmX0",
    "outputId": "6f20fbfd-515f-4332-8bdd-cb71e3031dc9"
   },
   "outputs": [],
   "source": [
    "# filter the data on column fWidth where fWidth > 60 and print the statistic information of 'fSize' (using describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3 Normalization and Standardization\n",
    "After reading the data and also filter the data, we will sometime need to perform the normalization and standardization on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J32BI09VoLVV",
    "outputId": "94a5f53f-c125-41ed-d98f-3ddaa8c1f54c"
   },
   "outputs": [],
   "source": [
    "# write code for the normalizing the fWidth on min max scaler and create new column 'scaled_fWidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiLfGTo4o711"
   },
   "outputs": [],
   "source": [
    "# How about we make a custom function to scale columns of our choice\n",
    "# write code for normalizaing multiple column in the same time \n",
    "def min_max_scaler(df, cols_to_scale):\n",
    "  # Takes a dataframe and list of columns to minmax scale. Returns a dataframe.\n",
    "  for col in cols_to_scale:\n",
    "    # Define min and max values and collect them\n",
    "    max_days = df.agg({col: 'max'}).collect()[0][0]\n",
    "    min_days = df.agg({col: 'min'}).collect()[0][0]\n",
    "    new_column_name = 'scaled_' + col\n",
    "    # Create a new column based off the scaled data\n",
    "    df = df.withColumn(new_column_name, \n",
    "                      (df[col] - min_days) / (max_days - min_days))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKaIsYvRpBaW",
    "outputId": "070bf742-c16e-476a-a68f-033bb49fc74b"
   },
   "outputs": [],
   "source": [
    "# try the above function you defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Sc3shCkrMAt"
   },
   "outputs": [],
   "source": [
    "# write code for the standardization for the fWidth and create new column 'standardized_fWidth ', save the dataframe called 'df_stand'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBjRs6nQrbNs",
    "outputId": "7b6a82de-dae2-46c0-8762-509e36f2a640"
   },
   "outputs": [],
   "source": [
    "# Check the mean to be close to 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNmjL8cAref8",
    "outputId": "8a47ac0a-aebd-49cd-eeea-6c4517c5d432"
   },
   "outputs": [],
   "source": [
    "# And the stddev to be close to 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7JUjUB0tzPU"
   },
   "source": [
    "### Task 1.4 Pyspark dataframe join\n",
    "Now we have to join the dataframe. Particularly, we want to join the original dataframe with 'df_stand'.\n",
    "Firstly, we need to find a ID to join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PAcbLEkArtOF",
    "outputId": "d6fec507-f7bd-4434-aa6f-6016a9aa2ab9"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Using windowfunction to create rownumber on both original dataframe and 'df_stand'\n",
    "windSpec = Window.orderBy(lit(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dIWic0xvQwF",
    "outputId": "1b9a71b2-42ff-498b-febb-ac10cec2be2e"
   },
   "outputs": [],
   "source": [
    "# Joining the two dataframe on rownumber and save the final joined dataframe as 'dataframe_pd'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtifCMJuy-Af",
    "outputId": "b6f7c01a-7ddb-4ff3-8d5f-570d6dcbe74d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5LJkcPFzPZ9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaBRs2xi1nIT"
   },
   "source": [
    "## Task 2 Exploratory Data Analysis\n",
    "### Task 2.1 Vislauzation for checking distribution \n",
    "Now we want to plot the histogram to check the distribution of 'fLength' from 'dataframe_pd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gIfXKiC2064T",
    "outputId": "c0e8d75e-fa32-448d-91ae-b98fdbed3e04"
   },
   "outputs": [],
   "source": [
    "# df.count() does not include NaN values\n",
    "df2 = dataframe_pd[[column for column in dataframe_pd if dataframe_pd[column].count() / len(dataframe_pd) >= 0.3]]\n",
    "del df2['row_count']\n",
    "print(\"List of dropped columns:\", end=\" \")\n",
    "for c in dataframe_pd.columns:\n",
    "    if c not in df2.columns:\n",
    "        print(c, end=\", \")\n",
    "print('\\n')\n",
    "dataframe_pd = df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgqoPr8z1t2S"
   },
   "source": [
    "Now lets take a look at how the fLength is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "wiv-R-Zn1qaj",
    "outputId": "42a72e45-9162-4404-d2da-c206810afb53"
   },
   "outputs": [],
   "source": [
    "#Using sns.distplot to plot the histogram for fLength\n",
    "print(dataframe_pd['fLength'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfyZx2il2YeV"
   },
   "source": [
    "Using pandas dataframe hist() function to plot the distributoins for all the numerical columns\n",
    "To do so lets first list all the types of our data from our dataset and take only the numerical ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "id": "tLJHllx714aC",
    "outputId": "89854ce6-5111-4e4f-92cd-022b9d7b3105"
   },
   "outputs": [],
   "source": [
    "print(list(set(dataframe_pd.dtypes.tolist())))\n",
    "df_num = dataframe_pd.select_dtypes(include = ['float64', 'int64'])\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "K9Bxg8SQ267y",
    "outputId": "c4540cd6-7fba-4086-9238-da22af03c922"
   },
   "outputs": [],
   "source": [
    "df_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8); # ; avoid having the matplotlib verbose informations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Boxplot\n",
    "Now we have to draw the boxplot to check the distribution on discrete columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "-35UbfNR5H-0",
    "outputId": "f4f89554-585a-4a9a-a3d2-99ba07edd751"
   },
   "outputs": [],
   "source": [
    "# Discrete the 'fWidth', 'fLength', 'fAlpha' and create new columns (bin is given in notebook)\n",
    "columns = ['fLength','fWidth','fAlpha']\n",
    "for col in columns:\n",
    "  new_column_name = col+'_bin'\n",
    "  bins = [0, 25, 50, 75, 100]\n",
    "  labels = [1,2,3,4]\n",
    "  dataframe_pd[new_column_name] = pd.cut(dataframe_pd[col], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sns.countplot to plot the boxplot for discrete 'fLength'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing the sns.countplot with x on discrete 'fLength' and y on the 'fConc' (numerical format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 (Advanced) Heatmap visualization\n",
    "We want to group the discrete 'fWidth', 'fLength' to obtain the count for each. However we want to exclude the 'fLength' on bin 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "gl9UzVfF5Q7c",
    "outputId": "a1a5bede-f2e3-4e29-9ddf-fa0c70c9c7dc"
   },
   "outputs": [],
   "source": [
    "# Create the group by with pandas\n",
    "type_grouped = dataframe_pd[dataframe_pd['fLength_bin']>1].groupby(['fLength_bin', 'fWidth_bin']).size()\n",
    "print(type_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "NzbyRXip52Yr",
    "outputId": "7536c04e-5470-4e8c-d07f-84f6405a07b9"
   },
   "outputs": [],
   "source": [
    "# Draw the count into heatmap by using ```sns.heatmap()```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M04PracClass-C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
