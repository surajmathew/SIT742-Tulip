{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESOYxiYmt7aC"
   },
   "source": [
    "\n",
    "# Practice Lab M03 (Version 2)\n",
    "## Data Acquisition and Data Source\n",
    "\n",
    "This task on data acquisition and data source is mainly to focus on how to properly acquire and read the data with differnt formats.\n",
    "\n",
    "To do:\n",
    "\n",
    "*   Read the given data for required format; \n",
    "*   Using Numpy and Pandas to do the ETL for the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRjF0oFYztMp"
   },
   "source": [
    "**Task 1.1** We first read the given data source. However, it is not in a standard readable format. The parquet file is in columnar storage format for efficient storage purpose (very common in cloud service scenario). Our goal in this task is to read this parquet file.\n",
    "\n",
    "**Background:**\n",
    "Parquet is an open source file format available to any project in the Hadoop ecosystem. Apache Parquet is designed for efficient as well as performant flat columnar storage format of data compared to row based files like CSV or TSV files. -- [check link](https://databricks.com/glossary/what-is-parquet)\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "There are many ways to read the parquet file:\n",
    "\n",
    "\n",
    "1.   Using pyarrow engine to read   \n",
    "        ```\n",
    "        ***pyarrow.parquet.ParquetFile(filename)***\n",
    "        ```\n",
    "2.   Directly read parquet with pandas  \n",
    "        ```\n",
    "       ***pandas.read_parquet(filename, engine='pyarrow')***\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6EUXOxC8TYg"
   },
   "outputs": [],
   "source": [
    "# read the parquet file by using pyarrow \n",
    "import pyarrow.parquet as pq\n",
    "#parquet_file = pq.ParquetFile('data/player.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9EbsD_J9dMY",
    "outputId": "e337fa3b-d00a-4ecf-c733-e7ce14800d48"
   },
   "outputs": [],
   "source": [
    "# print the parquet file schema to check the columns and its datatype\n",
    "#parquet_file.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTQNbJ8v_oCQ",
    "outputId": "10b10811-5b4c-437b-c3b9-2ba33b6bbb5b"
   },
   "outputs": [],
   "source": [
    "# print the metadata for the parquet file\n",
    "#parquet_file.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWzs-ofl-b1A"
   },
   "outputs": [],
   "source": [
    "# convert the parquet file into table format and print it in dataframe type\n",
    "table = parquet_file.read()\n",
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_FMMSTN-eOo"
   },
   "outputs": [],
   "source": [
    "#print the table into dictionary format. comment the %%capture to see the print output\n",
    "%%capture\n",
    "dic = table.to_pydict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "rcPivgPYrdcV",
    "outputId": "ab40f31e-d5f2-461d-e3eb-0d7afd6d6ce7"
   },
   "outputs": [],
   "source": [
    "# Directly read the parquet file by uing pandas\n",
    "import pandas as pd\n",
    "df_ = pd.read_parquet('data/player.parquet', engine='pyarrow')\n",
    "df_.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlneoNaiCU_e"
   },
   "source": [
    "**Task 1.2** After having the dataframe for the given data, let's first focus on the dictionary format -- which is the common used format while for semi-structure data in frontend development. **A record in dictionary  (one row in csv format) is now partitioned on column level**. \n",
    "We will try to format the dictionary by only selecting the **records (rows)** with Height(CM) > 165 and Height(CM) < 175  and then store it as a json file.\n",
    "\n",
    "**Background:** \n",
    "\n",
    "Dictionaries are used to store data values in key:value pairs.\n",
    "A dictionary is a collection which is ordered*, changeable and does not allow duplicates. -- [check link](https://www.w3schools.com/python/python_dictionaries.asp)\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "```\n",
    "for key, val in d.items():\n",
    "    if filter_string not in key:\n",
    "        continue\n",
    "    do something\n",
    "\n",
    "```\n",
    "\n",
    "or \n",
    "```\n",
    "filtered_dict = {k:v for (k,v) in d.items() if filter_string in k}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bzhRy5sGw5w",
    "outputId": "e149fa96-c358-41df-be17-ce301fc35b21"
   },
   "outputs": [],
   "source": [
    "# Let's first print out all the keys and the length of the values in the given data dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiTELvRQM1Vl",
    "outputId": "02d22c1f-2fde-494c-c7f4-fd82547ebb0e"
   },
   "outputs": [],
   "source": [
    "# let's print the keys and also the unique value from the values in in each (k,v) pair in the given data dictionary\n",
    "# unique value of the array could be calculated via numpy.unique(array,return_counts = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7MEor6d_HzY1",
    "outputId": "d80dbe14-6a42-475b-ab1b-ffea4b47da62"
   },
   "outputs": [],
   "source": [
    "# Let's then print both keys and the length of the unique value from the values in each (k,v) pair in the given data dictionary\n",
    "# for example, in the key value pair {fruit: ['apple','pear','banana']}, the length of the unique value is 3\n",
    "# unique value of the array could be calculated via numpy.unique(array,return_counts = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IY4mwSapI579"
   },
   "outputs": [],
   "source": [
    "# Now let's filter the dictionary, by doing it, you need to first create a null dictionary \n",
    "# and then write the filtered key value pair in the null dictionary\n",
    "# The code is given as below\n",
    "\n",
    "newDict = dict()\n",
    "for k,v in dic.items():\n",
    "   # Check if value meets the condition on particular key\n",
    "    if k == 'Height(CM)':\n",
    "        flt = filter(lambda height: height < 175 and height >165, np.array(v).astype(int))\n",
    "        new_v = list(flt)\n",
    "        newDict[k] = new_v\n",
    "    else:\n",
    "        newDict[k] = v\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UwGAzDhL4td",
    "outputId": "647bbe6f-a67a-4d84-f6db-2fff2689e522"
   },
   "outputs": [],
   "source": [
    "# let's double check the results\n",
    "for k, v in newDict.items():\n",
    "  print(k, np.unique(v,return_counts=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRhcaCCzNIDH"
   },
   "source": [
    "**Question here for 1.2** Have we finished the task 1.2? if yes, why? if no, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtcgScf2Npld"
   },
   "outputs": [],
   "source": [
    "# Now let's redo the filtering on the dictionary, by doing it, you need to first create a null dictionary \n",
    "# and then write the filtered key value pair in the null dictionary\n",
    "# Please write code as below again:\n",
    "\n",
    "newDict = dict()\n",
    "index_col = [i for i in range(len(dic['Height(CM)'])) if (dic['Height(CM)'][i] > 160) and (dic['Height(CM)'][i] <175)]\n",
    "for k,v in dic.items():   \n",
    "    newDict[k] = list(np.array(v)[index_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kYq02MHPCEt",
    "outputId": "9c507f8c-8f3d-4f67-c27f-072a21082223"
   },
   "outputs": [],
   "source": [
    "#let's check the length of the values again, do we see the difference?\n",
    "for k, v in newDict.items():\n",
    "  print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q83knHKOSZuz"
   },
   "outputs": [],
   "source": [
    "# let's store the filtered result into json,\n",
    "# the numpy to json encoder is provided\n",
    "import json\n",
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "BFFiZNUlTd_U",
    "outputId": "22e26d02-4be6-48a2-b8cd-1d2f9a97b449"
   },
   "outputs": [],
   "source": [
    "#let's read it via pandas to check\n",
    "df_json = pd.read_json('data/player.json')\n",
    "df_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdWCwUPHTvD7"
   },
   "source": [
    "**Task 1.3** After reading the data and also filter the dictionary, we finally have our json file and store it in our repo. However, if we need to regularly perform the similarly tasks for acquire the datasource, a parser is required. \n",
    "Could you write a parser function to contain the above parquet reading and dictionary filtering functionalities?\n",
    "\n",
    "**Background:** \n",
    "\n",
    "Python parser is mainly used for converting data in the required format, this conversion process is known as parsing. As in many different applications data obtained can have different data formats and these formats might not be suitable to the particular application and here comes the use of parser that means parsing is necessary for such situations. Therefore, parsing is generally defined as the conversion of data with one format to some other format is known as parsing -- [check link](https://www.educba.com/python-parser/)\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "```\n",
    "def parser(datasource_path,dataoutput_path,filter_key, min, max):\n",
    "    parquet_file = pq.ParquetFile(datasource_path)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IArKkmBTuGD"
   },
   "outputs": [],
   "source": [
    "#Code for the parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrWR-B-rWWUT"
   },
   "outputs": [],
   "source": [
    "# test the parser with condition on min=160, max=175 for age. Name the new json file as 'newplayer.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsvEDRCoXqs1"
   },
   "source": [
    "**Task 1.4** Now we have the parser and also have the json file, then next step for us is doing the ETL by using popular SQL. \n",
    "SQL is a domain-specific language used in programming and designed for managing data held in a relational database management system, or for stream processing in a relational data stream management system.\n",
    "In this task,\n",
    "we will review some simple sql query by using pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "iqppvTeocK7q",
    "outputId": "d7d63728-8efd-4edd-e7e3-61154831ef64"
   },
   "outputs": [],
   "source": [
    "# firstly, let's read the newplayer.json into dataframe by using pandas\n",
    "import pandas as pd\n",
    "df_newplayer = pd.read_json('data/newplayer.json')\n",
    "df_newplayer.columns = ['ID','Height', 'Weight', \n",
    "                      'Crossing', 'Finishing', 'HeadingAccuracy', \n",
    "                      'ShortPassing', 'Volleys', 'Dribbling', 'Curve',\n",
    "                      'FKAccuracy', 'LongPassing', 'BallControl', \n",
    "                      'Acceleration', 'SprintSpeed', 'Agility', \n",
    "                      'Reactions', 'Balance', 'ShotPower', 'Jumping', \n",
    "                      'Stamina', 'Strength', 'LongShots', 'Aggression', \n",
    "                      'Interceptions', 'Positioning', 'Vision', 'Penalties', \n",
    "                      'Composure', 'Marking', 'StandingTackle', 'SlidingTackle','__index_level_0__']\n",
    "df_newplayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vFBxKKFAgMU",
    "outputId": "b51dd973-67dd-4bb3-cc61-c2559ac77fe2"
   },
   "outputs": [],
   "source": [
    "# install pandasql\n",
    "!pip install -U pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "ZrIuWdVsBBPc",
    "outputId": "88a95fb5-b11e-4976-e15e-b293c6ba2280"
   },
   "outputs": [],
   "source": [
    "# let's first run a select query for the dataframe\n",
    "from pandasql import sqldf\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "query1 = pysqldf(\"SELECT * FROM df_newplayer LIMIT 10;\")\n",
    "query1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "cwpVugozBU0E",
    "outputId": "ac32bed0-dc5f-42df-c778-2f0500d49b2f"
   },
   "outputs": [],
   "source": [
    "# let's do some transformation on the column 'Dribbling' with conditions for df_newplayer\n",
    "# if 0<dribbling<=27 then 'low'; 27<dribbling<=61 then 'medium'; 61<dribbling<=73 then 'good'; 73<dribbling<=100 then 'excellent';\n",
    "query2 = pysqldf(\"\"\"\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "query2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "gKdf4yk3DyI3",
    "outputId": "6a7187f9-7fce-4300-f500-ea2bd3319279"
   },
   "outputs": [],
   "source": [
    "# let's calculate the average value of 'Crossing', 'Finishing', 'HeadingAccuracy', \n",
    "# 'ShortPassing','Dribbling', 'Curve',\n",
    "# 'FKAccuracy', 'LongPassing', 'BallControl'\n",
    "#  by grouping the height and weight\n",
    "\n",
    "query3 = pysqldf(\"\"\"\n",
    "\n",
    "\"\"\")\n",
    "query3.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZoNmP_ihVws"
   },
   "source": [
    "**Task 1.5** Could we do the similar ETL as task 1.3 in pandas?\n",
    "Such as group by the height and weight by having the average value of 'Crossing', 'Finishing', 'HeadingAccuracy', \n",
    "'ShortPassing','Dribbling', 'Curve',\n",
    "'FKAccuracy', 'LongPassing', 'BallControl'.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "using groupby() from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpVfYCMJiCMx"
   },
   "outputs": [],
   "source": [
    "# firstly, let's select the columns from df_newplayer\n",
    "df_sel = df_newplayer[['Height','Weight','Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing','Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObpCyGcui8y7"
   },
   "outputs": [],
   "source": [
    "# then let's create the aggregation by using pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "QL34qSctjQla",
    "outputId": "f46c03e1-6c40-4fa6-e04b-fe5b5cb519c0"
   },
   "outputs": [],
   "source": [
    "# let's reset the index to format the aggregated dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "iyn2nsjSkFNh",
    "outputId": "dbcfbe5f-3af6-4d0b-9455-7c8695932fdc"
   },
   "outputs": [],
   "source": [
    "# then let's convert BallControl and Dribbling to categorical data type\n",
    "# the conditions for dribbing are if 0<dribbling<=27 then 'low'; 27<dribbling<=61 then 'medium'; 61<dribbling<=73 then 'good'; 73<dribbling<=100 then 'excellent';\n",
    "# the conditions for ballcontrol are if 0<dribbling<=50 then 'low'; 50<dribbling<=65 then 'medium'; 65<dribbling<=75 then 'good'; 75<dribbling<=100 then 'excellent';\n",
    "\n",
    "conditions1 = [\n",
    "    (df_sel['BallControl']>0) & (df_sel['BallControl']<=50),\n",
    "    (df_sel['BallControl']>50) & (df_sel['BallControl']<=65),\n",
    "    (df_sel['BallControl']>65) & (df_sel['BallControl']<=75),\n",
    "    (df_sel['BallControl']>75) & (df_sel['BallControl']<=100)\n",
    "]\n",
    "\n",
    "conditions2 = [\n",
    "    (df_sel['Dribbling']>0) & (df_sel['Dribbling']<=27),\n",
    "    (df_sel['Dribbling']>27) & (df_sel['Dribbling']<=61),\n",
    "    (df_sel['Dribbling']>61) & (df_sel['Dribbling']<=73),\n",
    "    (df_sel['Dribbling']>73) & (df_sel['Dribbling']<=100)\n",
    "]\n",
    "\n",
    "choices = ['low','medium','good','excellent']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSADa42UkV3g"
   },
   "source": [
    "**Task 1.6 (Advanced)** Now we have the dataframe on both numerical and categorical datatype. For many big analysis, category datatype is not the best format to start with. The common way to deal category datatype is to transform it to one hot encode format (only with 1 and 0). Could you finish the one hot encode transforming for categorical column by using the provided code in hint?\n",
    "\n",
    "**Background:**\n",
    "\n",
    "One-hot Encoding is a type of vector representation in which all of the elements in a vector are 0, except for one, which has 1 as its value, where 1 represents a boolean specifying a category of the element. -- [check link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "```\n",
    "df_onehot = pd.get_dummies(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "He-VDqm5RP8B"
   },
   "outputs": [],
   "source": [
    "# then let's find and remove the numerical columns from all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "5eJL23JdkVDj",
    "outputId": "f5b3d3e9-7c6f-4308-958d-3f98d11b3ed4"
   },
   "outputs": [],
   "source": [
    "# let's use the code in hint to do the one hot encode and print the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "9XnnuI2jnNSw",
    "outputId": "2e3e37f0-73c0-42c3-a28e-9aba63007dac"
   },
   "outputs": [],
   "source": [
    "# let's combine the new onehot encode dataframe with numerical dataframe to obtain the full dataframe df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b18WQBJWngUl"
   },
   "source": [
    "##**Tasks 2 Advanced Data Acquisition **\n",
    "\n",
    "This task on Advanced data acquisition is to use numpy and pandas to perform more advanced Code-based ETL.\n",
    "\n",
    "To do:\n",
    "\n",
    "*   Create a function to calculate the euclidean distance between recoard; \n",
    "*   Find the most similar record for each one in the bank data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKV9ocD0odVQ"
   },
   "source": [
    "**Task 2.1** In numpy, the euclidean distance could be calculated via \n",
    "```\n",
    "np.sqrt(np.sum(np.square(point1 - point2)))\n",
    "```\n",
    "point1 and point2 is the 1D array, please folllow the above calculation and build a function to calculate the euclidean distance for any two arrays from a given dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8CqplgpoXuQ"
   },
   "outputs": [],
   "source": [
    "# define the funtion as below with name \"dist_func\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFB7noOktiV6"
   },
   "outputs": [],
   "source": [
    "# what about if point2 is a 2d array? how to calculate the distance from point1 to each dimension of point2?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeLpRt27qyaP"
   },
   "source": [
    "**Task 2.2** Now, we will need to calculate the euclidean distance between each row and all the rows (let's include the current row at here), also we would like to save the distances into array for each row. To the end, you will have a distance matrix with shape of (n,n) where n is the total rows.\n",
    "We will use *df_all* as the input.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Use the for loop on each row could be a good start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_yD02FtqYbs"
   },
   "outputs": [],
   "source": [
    "# let's write the code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqXg75YIv2JP",
    "outputId": "6b9c9c48-ea2c-4123-b51d-6823c757674a"
   },
   "outputs": [],
   "source": [
    "# let's print the distance matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6NrYStXwIg_"
   },
   "outputs": [],
   "source": [
    "# let's find the index of the smallest distance for each row in the distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "bQnvYx2_x3lu",
    "outputId": "e500c5e8-ef45-43ff-b7a3-fd2955f0036b"
   },
   "outputs": [],
   "source": [
    "#let's put the results into pandas dataframe\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M03PracClass-B.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
