{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESOYxiYmt7aC"
   },
   "source": [
    "\n",
    "# Practice Lab M03 (Version 3)\n",
    "## Data Acquisition and Data Source\n",
    "\n",
    "This task on data acquisition and data source is mainly to focus on how to properly acquire and read the data with differnt formats.\n",
    "\n",
    "To do:\n",
    "\n",
    "*   Read the given data for required format; \n",
    "*   Using Numpy and Pandas to do the ETL for the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRjF0oFYztMp"
   },
   "source": [
    "**Task 1.1** We first do a simple task on crawling the data from html -- the Deakin IT staff page. To do this, we need to understand the structure of the html as well as the web crawling library -- selenium or beautiful soup. \n",
    "In this task, please find all professors (Emeritus Professors, Professors and Associate Professors only) in Schoolf of IT and save it as csv \n",
    "\n",
    "**Background:**\n",
    "Selenium Python bindings provides a simple API to write functional/acceptance tests using Selenium WebDriver. -- [check link](https://selenium-python.readthedocs.io/installation.html#introduction)\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "There are several steps to crawl the HTML:\n",
    "\n",
    "\n",
    "1.   Building a web driver from Selenium and use the get function to call the page.  \n",
    "        ```\n",
    "        wd = webdriver.Chrome('chromedriver',options=options)\n",
    "        ***wd.get(\"https://www.deakin.edu.au/information-technology/staff-listing\")***\n",
    "        ```\n",
    "2.   Find the table in the page and then further format the content into csv \n",
    "        ```\n",
    "       shapes = wd.find_elements_by_tag_name('table')\n",
    "       for table in shapes[:3]:\n",
    "          for tr in table.find_elements_by_tag_name('tr'):\n",
    "              for td in tr.find_elements_by_tag_name('td'):\n",
    "                  for a in td.find_elements_by_tag_name('a'):\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k6EUXOxC8TYg",
    "outputId": "90f11852-7214-4faa-f74b-3cbed35333ee"
   },
   "outputs": [],
   "source": [
    "# install chromium, its driver, and selenium\n",
    "!apt-get update\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "!pip install selenium\n",
    "# set options to be headless, ..\n",
    "from selenium import webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "# open it, go to a website, and get results\n",
    "wd = webdriver.Chrome('chromedriver',options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9EbsD_J9dMY",
    "outputId": "eb003b14-eb7d-4872-9b5a-b76e48787be6"
   },
   "outputs": [],
   "source": [
    "# print the content from the tables which contain the professors information\n",
    "wd.get(\"https://www.deakin.edu.au/information-technology/staff-listing\")\n",
    "shapes = wd.find_elements_by_tag_name('table')\n",
    "for table in shapes[:3]:\n",
    "  for tr in table.find_elements_by_tag_name('tr'):\n",
    "    for td in tr.find_elements_by_tag_name('td'):\n",
    "        for a in td.find_elements_by_tag_name('a'):\n",
    "            print(a.get_attribute('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTQNbJ8v_oCQ"
   },
   "outputs": [],
   "source": [
    "# We need to create a parser to seperate the information, for example, we need to extract the title and name for each professor\n",
    "def parse_name(stringtext):\n",
    "  return \" \".join(stringtext.split(\" \")[-2:]),\" \".join(stringtext.split(\" \")[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWzs-ofl-b1A",
    "outputId": "96a28965-0802-45b3-87a8-241895e357bf"
   },
   "outputs": [],
   "source": [
    "# let's store the name and title into a new dataframe.\n",
    "import pandas as pd\n",
    "data = []\n",
    "for table in shapes[:3]:\n",
    "  for tr in table.find_elements_by_tag_name('tr'):\n",
    "    #tds = tr.find_elements_by_tag_name('td')\n",
    "    for td in tr.find_elements_by_tag_name('td'):\n",
    "        for a in td.find_elements_by_tag_name('a'):\n",
    "          data.append(parse_name(a.get_attribute('text')))\n",
    "df = pd.DataFrame(data)\n",
    "df['University'] = 'Deakin University'\n",
    "df.columns = ['Name','Title','University']\n",
    "df.to_csv('data/Professor_name_list.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_FMMSTN-eOo"
   },
   "outputs": [],
   "source": [
    "#Below code is the function to fetch the google scholar profile of each professor\n",
    "#What it does are: \n",
    "#1 searching the google scholar for all professors to obtain their citations_all, h-index_all, i10_all, citation_since2016 h-index-2016 and i10_since2016.\n",
    "#2 Saving the results as csv and must having all the professors name, title and all 6 citation information.\n",
    "#3 If the professors did not have the google scholar profle with the name, save the 6 citation information as string \"na\"\n",
    "#4 The input of the function below is in a format of ('firstname+lastname+deakin+university') -- which is a preferable format for google scholar to search\n",
    "from time import sleep\n",
    "def fetch_google_scholar_profile(input):\n",
    "  statistics = []\n",
    "  url = str(\"https://scholar.google.com/citations?hl=en&view_op=search_authors&mauthors=\"+input+\"&btnG=\")\n",
    "  wd.get(url)\n",
    "  #sleep(5)\n",
    "  bodyText = wd.find_element_by_tag_name(\"body\").text\n",
    "  if \"didn't match any user profiles\" in bodyText:\n",
    "    statistics.extend(['na', 'na', 'na','na','na','na'])\n",
    "  else:\n",
    "    elems = wd.find_element_by_class_name(\"gs_ai_pho\")\n",
    "    profile_url = elems.get_attribute(\"href\")\n",
    "    wd.get(profile_url)\n",
    "    table = wd.find_element_by_id(\"gsc_rsb_st\")\n",
    "    for td in table.find_elements_by_class_name('gsc_rsb_std'):\n",
    "      #for td in tr.find_elements_by_tag_name('td'):\n",
    "      #  a = td.find_element_by_tag_name('a')\n",
    "      #tds = tr.find_elements_by_class_name('gsc_rsb_std')\n",
    "      statistics.append(td.get_attribute('textContent'))\n",
    "  return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcPivgPYrdcV",
    "outputId": "df6325d3-9346-43e5-b291-989965931f43"
   },
   "outputs": [],
   "source": [
    "#Now, let's collect the citation information for all professors we have crawled.\n",
    "citation_info = []\n",
    "for i in range(len(df.Name)):\n",
    "  author = df.Name[i].split(\" \",1)[0]+\"+\"+df.Name[i].split(\" \",1)[1]+\"+deakin+\"+\"university\"\n",
    "  print(author)\n",
    "  citation = fetch_google_scholar_profile(author)\n",
    "  citation.insert(0, df.Title[i])\n",
    "  citation.insert(0, df.Name[i])\n",
    "  citation_info.append(citation)\n",
    "  print('finished') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4M0uz5G0bWgX"
   },
   "outputs": [],
   "source": [
    "#Let's store the citation information into csv \n",
    "import numpy as np\n",
    "df_citation = pd.DataFrame(np.array(citation_info))\n",
    "df_citation.columns = ['Name','Title','citation_all','citation_since2016','h-index_all','h-index_since2016','i10-index_all','i10-index_since2016']\n",
    "df_citation.to_csv('data/Professor_citation_informaton.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlneoNaiCU_e"
   },
   "source": [
    "**Task 1.2** After acquiring the dataframe for the html, let's first focus on the dictionary format -- which is the common used format while for semi-structure data in frontend development. **A record in dictionary  (one row in csv format) is now partitioned on column level**. \n",
    "We will try to first convert the Professor_citation_informaton.csv to \n",
    "the in memory dictionary (removing 'na'). Then we will \n",
    "format the dictionary by only selecting the **records (rows)** with citation_all > 3000 and citation_all < 10000  and then store it as a json file.\n",
    "\n",
    "**Background:** \n",
    "\n",
    "Dictionaries are used to store data values in key:value pairs.\n",
    "A dictionary is a collection which is ordered*, changeable and does not allow duplicates. -- [check link](https://www.w3schools.com/python/python_dictionaries.asp)\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "```\n",
    "for key, val in d.items():\n",
    "    if filter_string not in key:\n",
    "        continue\n",
    "    do something\n",
    "\n",
    "```\n",
    "\n",
    "or \n",
    "```\n",
    "filtered_dict = {k:v for (k,v) in d.items() if filter_string in k}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7nQCxP2cTjY"
   },
   "outputs": [],
   "source": [
    "#Firstly, we will remove the 'na' from the df_citation and convert it to dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bzhRy5sGw5w",
    "outputId": "f4fa694f-ba9e-4073-e9de-2588db0a0646"
   },
   "outputs": [],
   "source": [
    "# Let's first print out all the keys and the length of the values in the given data dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiTELvRQM1Vl",
    "outputId": "7dec2c8c-c76e-4bf0-b0f5-04bf3dad7f68"
   },
   "outputs": [],
   "source": [
    "# let's print the keys and also the unique value from the values in in each (k,v) pair in the given data dictionary\n",
    "# unique value of the array could be calculated via numpy.unique(array,return_counts = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7MEor6d_HzY1",
    "outputId": "36683423-7bc1-42b1-aff6-88ac0d4b4979"
   },
   "outputs": [],
   "source": [
    "# Let's then print both keys and the length of the unique value from the values in each (k,v) pair in the given data dictionary\n",
    "# for example, in the key value pair {fruit: ['apple','pear','banana']}, the length of the unique value is 3\n",
    "# unique value of the array could be calculated via numpy.unique(array,return_counts = False)\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IY4mwSapI579"
   },
   "outputs": [],
   "source": [
    "# Now let's filter the dictionary, by doing it, you need to first create a null dictionary \n",
    "# and then write the filtered key value pair in the null dictionary\n",
    "# The code is given as below\n",
    "\n",
    "newDict = dict()\n",
    "for k,v in dic.items():\n",
    "   # Check if value meets the condition on particular key\n",
    "    if k == 'citation_all':\n",
    "        flt = filter(lambda citation: citation < 10000 and citation >3000, np.array(v).astype(int))\n",
    "        new_v = list(flt)\n",
    "        newDict[k] = new_v\n",
    "    else:\n",
    "        newDict[k] = v\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UwGAzDhL4td",
    "outputId": "1a50f21f-bd97-490f-fd01-2ab5678d2afe"
   },
   "outputs": [],
   "source": [
    "# let's double check the results\n",
    "for k, v in newDict.items():\n",
    "  print(k, np.unique(v,return_counts=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRhcaCCzNIDH"
   },
   "source": [
    "**Question here for 1.2** Have we finished the task 1.2? if yes, why? if no, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtcgScf2Npld"
   },
   "outputs": [],
   "source": [
    "# Now let's redo the filtering on the dictionary, by doing it, you need to first create a null dictionary \n",
    "# and then write the filtered key value pair in the null dictionary\n",
    "# Please write code as below again:\n",
    "\n",
    "newDict = dict()\n",
    "index_col = [i for i in range(len(dic['citation_all'])) if (np.array(dic['citation_all']).astype('int')[i] > 3000) and (np.array(dic['citation_all']).astype('int')[i] <10000)]\n",
    "for k,v in dic.items():   \n",
    "    newDict[k] = list(np.array(v)[index_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kYq02MHPCEt",
    "outputId": "8e401636-c878-4c7b-db51-e2faff512623"
   },
   "outputs": [],
   "source": [
    "#let's check the length of the values again, do we see the difference?\n",
    "for k, v in newDict.items():\n",
    "  print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q83knHKOSZuz"
   },
   "outputs": [],
   "source": [
    "# let's store the filtered result into json,\n",
    "# the numpy to json encoder is provided\n",
    "import json\n",
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()\n",
    "\n",
    "with open('data/citation.json', 'w') as fp:\n",
    "    json.dump(newDict, fp, default=np_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "BFFiZNUlTd_U",
    "outputId": "84f8a049-f563-4f8c-a250-ade9fc2213bb"
   },
   "outputs": [],
   "source": [
    "#let's read it via pandas to check\n",
    "df_json = pd.read_json('data/citation.json')\n",
    "df_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsvEDRCoXqs1"
   },
   "source": [
    "**Task 1.3** Now we have the the json file, then next step for us is doing the ETL by using popular SQL. \n",
    "SQL is a domain-specific language used in programming and designed for managing data held in a relational database management system, or for stream processing in a relational data stream management system.\n",
    "In this task,\n",
    "we will review some simple sql query by using pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "iqppvTeocK7q",
    "outputId": "0a8572b8-f9b1-437f-81e9-afd548b685f9"
   },
   "outputs": [],
   "source": [
    "# firstly, let's read the newplayer.json into dataframe by using pandas\n",
    "import pandas as pd\n",
    "df_citation = pd.read_json('data/citation.json')\n",
    "df_citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vFBxKKFAgMU",
    "outputId": "381a9491-1b8d-4f98-d208-9dd66e3f0a5c"
   },
   "outputs": [],
   "source": [
    "# install pandasql\n",
    "!pip install -U pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "ZrIuWdVsBBPc",
    "outputId": "09d85e75-19ed-48ef-eae7-5d6dfed1e2ce"
   },
   "outputs": [],
   "source": [
    "# let's first run a select query for the dataframe\n",
    "from pandasql import sqldf\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "query1 = pysqldf(\"SELECT * FROM df_citation LIMIT 10;\")\n",
    "query1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "cwpVugozBU0E",
    "outputId": "eeb16b45-5a09-4f2a-9b0e-b6790c2f906c"
   },
   "outputs": [],
   "source": [
    "# let's do some transformation on the column 'citation_all' with conditions for df_newplayer\n",
    "# if 0<citation_all<=4000 then 'low'; 4000<citation_all<=4300 then 'medium'; 4300<citation_all<=7500 then 'good'; 7500<citation_all<=10000 then 'excellent';\n",
    "query2 = pysqldf(\"\"\"\n",
    "\n",
    "\"\"\")\n",
    "query2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "gKdf4yk3DyI3",
    "outputId": "7054bce0-fb36-44e1-9b6b-0005d36666e2"
   },
   "outputs": [],
   "source": [
    "# let's calculate the average value of 'citation_all', 'citation_since2016' by group the Title\n",
    "\n",
    "query3 = pysqldf(\"\"\"\n",
    "\n",
    "\"\"\")\n",
    "query3.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZoNmP_ihVws"
   },
   "source": [
    "**Task 1.4** Could we do the similar ETL as task 1.3 in pandas?\n",
    "Such as group by the Name and Title by having the average value of 'citation_all', 'citation_since2016'\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "using groupby() from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpVfYCMJiCMx"
   },
   "outputs": [],
   "source": [
    "# firstly, let's select the columns from df_citation\n",
    "df_sel = df_citation[['Name','Title','citation_all', 'citation_since2016']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObpCyGcui8y7"
   },
   "outputs": [],
   "source": [
    "# then let's create the aggregation by using pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "QL34qSctjQla",
    "outputId": "8cd25ea6-edb5-4c67-cd98-fe5328df1dfb"
   },
   "outputs": [],
   "source": [
    "# let's reset the index to format the aggregated dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "iyn2nsjSkFNh",
    "outputId": "199a6131-1458-4443-f16e-8312a1f5695e"
   },
   "outputs": [],
   "source": [
    "# then let's convert citation_all to categorical data type\n",
    "# the conditions for citation_all are if 0<citation_all<=4000 then 'low'; 4000<citation_all<=4300 then 'medium'; 4300<citation_all<=7500 then 'good'; 7500<citation_all<=10000 then 'excellent';\n",
    "\n",
    "conditions1 = [\n",
    "    (df_sel['citation_all']>0) & (df_sel['citation_all']<=4000),\n",
    "    (df_sel['citation_all']>4000) & (df_sel['citation_all']<=4300),\n",
    "    (df_sel['citation_all']>4300) & (df_sel['citation_all']<=7500),\n",
    "    (df_sel['citation_all']>7500) & (df_sel['citation_all']<=10000)\n",
    "]\n",
    "\n",
    "choices = ['low','medium','good','excellent']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSADa42UkV3g"
   },
   "source": [
    "**Task 1.5 (Advanced)** Now we have the dataframe on both numerical and categorical datatype. For many big analysis, category datatype is not the best format to start with. The common way to deal category datatype is to transform it to one hot encode format (only with 1 and 0). Could you finish the one hot encode transforming for categorical column by using the provided code in hint?\n",
    "\n",
    "**Background:**\n",
    "\n",
    "One-hot Encoding is a type of vector representation in which all of the elements in a vector are 0, except for one, which has 1 as its value, where 1 represents a boolean specifying a category of the element. -- [check link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "```\n",
    "df_onehot = pd.get_dummies(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "He-VDqm5RP8B"
   },
   "outputs": [],
   "source": [
    "# then let's find and remove the numerical columns from all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "5eJL23JdkVDj",
    "outputId": "cf9e5437-b4a2-45c1-e16f-b06a46d4a069"
   },
   "outputs": [],
   "source": [
    "# let's use the code in hint to do the one hot encode and print the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "9XnnuI2jnNSw",
    "outputId": "54568c66-a2cb-441d-dfec-c5a133567feb"
   },
   "outputs": [],
   "source": [
    "# let's combine the new onehot encode dataframe with numerical dataframe to obtain the full dataframe df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b18WQBJWngUl"
   },
   "source": [
    "##**Tasks 2 Advanced Data Acquisition **\n",
    "\n",
    "This task on Advanced data acquisition is to use numpy and pandas to perform more advanced Code-based ETL.\n",
    "\n",
    "To do:\n",
    "\n",
    "*   Create a function to calculate the euclidean distance between recoard; \n",
    "*   Find the most similar record for each one in the bank data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKV9ocD0odVQ"
   },
   "source": [
    "**Task 2.1** In numpy, the euclidean distance could be calculated via \n",
    "```\n",
    "np.sqrt(np.sum(np.square(point1 - point2)))\n",
    "```\n",
    "point1 and point2 is the 1D array, please folllow the above calculation and build a function to calculate the euclidean distance for any two arrays from a given dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8CqplgpoXuQ"
   },
   "outputs": [],
   "source": [
    "# define the funtion as below with name \"dist_func\"\n",
    "\n",
    "def dist_func(row1,row2):\n",
    "  return np.sqrt(np.sum(np.square(row1 - row2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFB7noOktiV6"
   },
   "outputs": [],
   "source": [
    "# what about if point2 is a 2d array? how to calculate the distance from point1 to each dimension of point2?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeLpRt27qyaP"
   },
   "source": [
    "**Task 2.2** Now, we will need to calculate the euclidean distance between each row and all the rows (let's include the current row at here), also we would like to save the distances into array for each row. To the end, you will have a distance matrix with shape of (n,n) where n is the total rows.\n",
    "We will use *df_all* as the input.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Use the for loop on each row could be a good start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_yD02FtqYbs"
   },
   "outputs": [],
   "source": [
    "# let's write the code here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqXg75YIv2JP",
    "outputId": "e84d88ca-865f-4e8e-b54b-d4332ff57dea"
   },
   "outputs": [],
   "source": [
    "# let's print the distance matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6NrYStXwIg_"
   },
   "outputs": [],
   "source": [
    "# let's find the index of the smallest distance for each row in the distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "bQnvYx2_x3lu",
    "outputId": "da770a0b-0133-4599-e197-07f2f9b4030a"
   },
   "outputs": [],
   "source": [
    "#let's put the results into pandas dataframe\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M03PracClass-C.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
