{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbeZa7WS_2-s"
   },
   "source": [
    "![Cloud-First](../image/CloudFirst.png) \n",
    "\n",
    "\n",
    "# SIT742: Modern Data Science\n",
    "**(Module: Big Data)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, change and distribute this package.\n",
    "- If you found any issue/bug for this document, please submit an issue at [tulip-lab/sit742](https://github.com/tulip-lab/sit742/issues)\n",
    "\n",
    "\n",
    "Prepared by **SIT742 Teaching Team**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Session 3E: Data Acquisition (2)\n",
    "\n",
    "In this session, we will learn how to use Python Packages to ETL the data and files.\n",
    "\n",
    "\n",
    "\n",
    "### Content\n",
    "\n",
    "\n",
    "1. `Pandas` Basics\n",
    "\n",
    "2. Loading `CSV` Data\n",
    "\n",
    "3. Data Extraction through Web `API` \n",
    "\n",
    "4. Web Crawling using `BeautifulSoup`\n",
    "\n",
    "\n",
    "\n",
    "**Note**: The data available on those service might be changing, so you need to adjust the code to accommodate those changes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdDCBCLn_2-1"
   },
   "source": [
    "## Part 1. `Pandas` Basics\n",
    "\n",
    "\n",
    "`Pandas` is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "To get started, we can import `Pandas` with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Series` and `DataFrame`\n",
    "\n",
    "Two core components of pandas are the `Series` and `DataFrame`. A `Series` is essentially a column, and a `DataFrame` is a multi-dimensional table made up of a collection of `series`. For a two dimensional DataFrame, the row labels are referred to as `index`, and the column labels are referred to as `columns`.\n",
    "\n",
    "There are many ways to create a dataframe, and one common option is to use a simple dictionary with each entry acting as a column in the dataframe, as shown in below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining DataFrame by specifing a list of observations\n",
    "df_1= pd.DataFrame([['a', 'b', 'c'],\n",
    "                 ['d', 'e', 'f'],\n",
    "                 ['g', 'h', 'i']],\n",
    "                index = [1,2,3], columns = ['col1', 'col2', 'col3'])\n",
    "                \n",
    "# Defining DataFrame by specifing a dictionary of columns \n",
    "df_2=pd.DataFrame({'col1': ['a', 'd', 'g'], \n",
    "                 'col2': ['b', 'e', 'h'],\n",
    "                 'col3': ['c', 'f', 'i']}, \n",
    "                 index = [1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `df_1` and `df_2` defined above are identical.\n",
    "\n",
    "The index labels, columns labels, and data values stored in the dataframe `df_1` can be retrieved using `df_1.index`, `df_1.columns` and `df_1.values` respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.index\n",
    "df_1.columns\n",
    "df_1.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pandas` also provides operations to inspect a dataframe to gain better understanding of its contents.\n",
    "\n",
    "- `.head(n)` and `.tail(n)` returns the top and bottom `n` rows of the dataframe, respectively.\n",
    "- `.describe()` returns summary of all numerical variables (columns) in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head(2)\n",
    "df_1.tail(2)\n",
    "df_1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Loading `CSV` Data\n",
    "\n",
    "Here you will learn  how to use Pandas \n",
    "[read_csv()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) function to load a CSV file. Before we start importing our CSV file, it might be good for you to read [Pandas tutorial on reading CSV files](http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-csv-table).\n",
    "\n",
    "If `wget` was not installed in your `Python` platform, install it first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the `csv` data file is avilable at a URL, we use `wget` to download it to the local file system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Jupyter/data/Melbourne_bike_share.csv'\n",
    "\n",
    "DataSet = wget.download(link_to_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Importing `CSV` data\n",
    "\n",
    "Importing `CSV` files with `Pandas` function [`read_csv()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)  and converting the data into a form Python can understand is simple. It only takes a couple of lines of code. The imported data will be stored in Pandas `DataFrame`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csvdf = pd.read_csv(\"Melbourne_bike_share.csv\")\n",
    "type(csvdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Data\n",
    "\n",
    "Now, the data should be loaded into `Python`.  Let's have a look at the first 5 records in the dataset. There are a coupe of ways to retrieve these records.\n",
    "\n",
    "For example, you can use \n",
    "* `csvdf.head(n = 5)`: It will return first `n` rows in a DataFrame, `n = 5` by default.\n",
    "* `csvdf[:5]`: It uses the slicing method to retrieve the first `5` rows.\n",
    "\n",
    "Refer to \"[Indexing and Selecting Data](http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)\"\n",
    "for how to slice, dice, and generally get and set subsets of pandas objects.\n",
    "\n",
    "Here, we use the `head` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have an oveall inspection\n",
    "csvdf.describe()\n",
    "\n",
    "#We can use the head instead of the For Statement shown in the above example.\n",
    "csvdf.head()\n",
    "csvdf[:5]\n",
    "csvdf.loc[:4] \n",
    "\n",
    "#tail is used to show last several records.\n",
    "csvdf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the row indices are integers automatically generated by `Pandas`.\n",
    "Suppose you want to set IDs as row indices and delete the ID column.\n",
    "Resetting the row indices can be easily done with the following DataFrame functionï¼š\n",
    "\n",
    ">DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See its [API webpage](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html) \n",
    "for the detailed usage.\n",
    "The keys are going to be the IDs in the first column. \n",
    "By setting `inplace = True`, the corresponding change is done inplace and won't return a new DataFrame object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To show how many records in the file\n",
    "len(csvdf.ID.unique())\n",
    "\n",
    "csvdf.set_index(csvdf.ID, inplace = True)\n",
    "csvdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the ID column that is now redundant, you use DataFrame `drop` function and set `inplace = True`\n",
    ">DataFrame.drop(labels, axis=0, level=None, inplace=False, errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvdf.drop('ID', 1, inplace = True)\n",
    "csvdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TkOmnCyu_2-5"
   },
   "source": [
    "## Part 3. Data Extraction through Web `API` \n",
    "\n",
    "\n",
    "Many of you will probably be interested in scraping data from the web for your projects. For example, what if we were interested in working with some historical Canadian weather data? Well, we can get that from: http://climate.weather.gc.ca using their API. Requests are going to be formatted like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NaOZOhi0_2-8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url_template = \"http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&stationID=5415&Year={year}&Month={month}&timeframe=1&submit=Download+Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04ZKvRtM_2_J"
   },
   "source": [
    "Note that we've requested the data be returned as a CSV, and that we're going to supply the month and year as inputs when we fire off the query. To get the data for March 2012, we need to format it with month=3, year=2012:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_bhAPpd_2_M"
   },
   "outputs": [],
   "source": [
    "url = url_template.format(month=3, year=2012)\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o76kkUME_2_V"
   },
   "source": [
    "This is great! We can just use the same `read_csv` function as before, and just give it a URL as a filename. Awesome.\n",
    "\n",
    "Upon inspection, we find out that there are 0 rows (as in 03/2020) of metadata at the top of this CSV, but pandas knows CSVs are weird, so there's a `skiprows` options. We parse the dates again, and set 'Date/Time' to be the index column. Here's the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5lP8CF9_2_Y"
   },
   "outputs": [],
   "source": [
    "weather_mar2012 = pd.read_csv(url, skiprows=0, index_col='Date/Time (LST)', parse_dates=True, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u-uWpzab_2_g"
   },
   "outputs": [],
   "source": [
    "weather_mar2012.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M32ynyCR_2_p"
   },
   "source": [
    "As before, we can get rid of any columns that don't contain real data using ${\\tt .dropna()}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "czOBl_A8_2_s"
   },
   "outputs": [],
   "source": [
    "weather_mar2012 = weather_mar2012.dropna(axis=1, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4270mD1_2_1"
   },
   "outputs": [],
   "source": [
    "weather_mar2012.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O74UiyU4_3AA"
   },
   "source": [
    "Getting better! The Year/Month/Day/Time columns are redundant, though, and the Data Quality column doesn't look too useful. Let's get rid of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nybBYbyH_3AG"
   },
   "outputs": [],
   "source": [
    "weather_mar2012 = weather_mar2012.drop(['Year', 'Month', 'Day', 'Time (LST)'], axis=1)\n",
    "weather_mar2012[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fERnSYxB_3AQ"
   },
   "source": [
    "Great! Now let's figure out how to download the whole year? It would be nice if we could just send that as a single request, but like many APIs this one is limited to prevent people from hogging bandwidth. No problem: we can write a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_cCjj05_3AT"
   },
   "outputs": [],
   "source": [
    "def download_weather_month(year, month):\n",
    "    url = url_template.format(year=year, month=month)\n",
    "    weather_data = pd.read_csv(url, skiprows=0, index_col='Date/Time (LST)', parse_dates=True)\n",
    "    weather_data = weather_data.dropna(axis=1)\n",
    "    weather_data.columns = [col.replace('\\xb0', '') for col in weather_data.columns]\n",
    "    weather_data = weather_data.drop(['Year', 'Day', 'Month', 'Time (LST)'], axis=1)\n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9icj3vwY_3Ab"
   },
   "source": [
    "Now to test that this function does the right thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZT66AAk_3Ae"
   },
   "outputs": [],
   "source": [
    "download_weather_month(2020, 1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wax-OUr_3Am"
   },
   "source": [
    "Woohoo! Now we can iteratively request all the months using a single line. This will take a little while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-s22hxtp_3Ao"
   },
   "outputs": [],
   "source": [
    "data_by_month = [download_weather_month(2012, i) for i in range(1, 12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4S-Wk3j_3Ax"
   },
   "source": [
    "Once that's done, it's easy to concatenate all the dataframes together into one big dataframe using ${\\tt pandas.concat()}$. And now we have the whole year's data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJCpxgx6_3A1"
   },
   "outputs": [],
   "source": [
    "weather_2012 = pd.concat(data_by_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFEUd8lF_3A9"
   },
   "source": [
    "This thing is long, so instead of printing out the whole thing, I'm just going to print a quick summary of the ${\\tt DataFrame}$ by calling ${\\tt .info()}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQyvC2MG_3BA"
   },
   "outputs": [],
   "source": [
    "weather_2012.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckItOLd__3BJ"
   },
   "source": [
    "And a quick reminder, if we wanted to save that data to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLWc_X5h_3BM"
   },
   "outputs": [],
   "source": [
    "weather_2012.to_csv('weather_2012.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pxaDCOS1_3BY"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbZfj5eH_3Bl"
   },
   "source": [
    "And finally, something you should do early on in the wrangling process, plot data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mv6HNBvW_3Br"
   },
   "outputs": [],
   "source": [
    "# plot that data\n",
    "import matplotlib.pyplot as plt \n",
    "# so now 'plt' means matplotlib.pyplot\n",
    "dateRange = weather_2012.index\n",
    "temperature = weather_2012['Temp (C)']\n",
    "\n",
    "df1 = pd.DataFrame({'Temperature' : temperature}, index=dateRange)\n",
    "      \n",
    "plt.plot(df1.index.to_pydatetime(), df1.Temperature)\n",
    "plt.title(\"The 2012 annual temperature in Canada\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9ansQSO_3B8"
   },
   "outputs": [],
   "source": [
    "# nothing to see... in iPython you need to specify where the chart will display, usually it's in a new window\n",
    "# to see them 'inline' use:\n",
    "%matplotlib inline\n",
    "#If you add the %matplotlib inline, then you can skip the plt.show() function.\n",
    "#How to close python warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Tfyd2Hf_3CJ"
   },
   "outputs": [],
   "source": [
    "# that's better, try other plots, scatter is popular, also boxplott\n",
    "df1 = pd.read_csv('weather_2012.csv', low_memory=False)\n",
    "df1.plot(kind='scatter',x='Dew Point Temp (C)',y='Rel Hum (%)',color='red')\n",
    "df1.plot(kind='scatter',x='Temp (C)',y='Wind Spd (km/h)',color='yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZdrp7EAH6eM"
   },
   "outputs": [],
   "source": [
    "# show first several 'weather' columns value\n",
    "weather_2012['Weather'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVlfon_bEr60"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Boxplot sample\n",
    "climategroup1 = df1[df1['Weather']=='Fog']['Temp (C)']\n",
    "climategroup2 = df1[df1['Weather']=='Rain']['Temp (C)']\n",
    "climategroup3 = df1[df1['Weather']=='Clear']['Temp (C)']\n",
    "climategroup4 = df1[df1['Weather']=='Cloudy']['Temp (C)']\n",
    "\n",
    "data =[climategroup1,climategroup2,climategroup3,climategroup4]\n",
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Temperature Boxplot based on the Climate group')\n",
    "ax1.set_ylabel('Temperature')\n",
    "ax1.set_xlabel('Climate Group')\n",
    "\n",
    "boxplot=ax1.boxplot(data, \n",
    "                    notch=True,\n",
    "                    patch_artist=True,\n",
    "                    labels=['Fog','Rain','Clear','Cloudy'],\n",
    "                    boxprops=dict(linestyle='--', linewidth=2, color='black'))\n",
    "\n",
    "colors = ['cyan', 'pink', 'lightgreen', 'tan', 'pink']\n",
    "for patch, color in zip(boxplot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s5P7FONd_3CR"
   },
   "source": [
    "## Part 4. States and Territories of Australia \n",
    "\n",
    "We are interested in getting  State and Territory information from Wikipedia, however we do not want to copy and paste the table : )\n",
    "\n",
    "Here is the URL\n",
    "https://en.wikipedia.org/wiki/States_and_territories_of_Australia   \n",
    "\n",
    "We need two libraries to do the task:\n",
    "\n",
    "Check documentations here:\n",
    "* [urllib](https://docs.python.org/2/library/urllib.html)\n",
    "* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYlnsbPr_3CV"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info[0] == 3:\n",
    "    from urllib.request import urlopen\n",
    "else:\n",
    "    from urllib import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBmeWNVx_3Ce"
   },
   "source": [
    "We first save the link in wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7gpk9P-_3Cg"
   },
   "outputs": [],
   "source": [
    "wiki = \"https://en.wikipedia.org/wiki/States_and_territories_of_Australia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OpIxUZ4a_3Cn"
   },
   "source": [
    "Then use ulropen to open the page. \n",
    "\n",
    "If you get \"SSL: CERTIFICATE_VERIFY_FAILED\", what you need to do is find where \"Install Certificates.command\" file is, and click it to upgrade the certificate. Then, you should be able to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00UN2EMw_3Cq"
   },
   "outputs": [],
   "source": [
    "page = urlopen(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbjPbWLl_3Cx"
   },
   "outputs": [],
   "source": [
    "if sys.version_info[0] == 3:\n",
    "    page = page.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyy4lpJ7_3C4"
   },
   "source": [
    "You will meet BeautifulSoup later in this subject, so don't worry if you feel uncomfortable with it now. You can always revisit. \n",
    "\n",
    "We begin by reading in the source code and creating a Beautiful Soup object with the BeautifulSoup function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMjST_SK_3C9"
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arXCtPja_3DJ"
   },
   "source": [
    "Then we print and see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VUNMhBW_3DM"
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GNlxOnLL_3DX"
   },
   "source": [
    "For who do not know much about HTML, this might be a bit overwhelming, but essentially it contains lots of tags in the angled brackets providing structural and formatting information that we don't care so much here. What we need is the table. \n",
    "\n",
    "Let's first check the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OX2ce0ZK_3Df"
   },
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5X_vbyY_3Do"
   },
   "source": [
    "It looks fine, then we would like to find the table. \n",
    "\n",
    "Let's have a try to extract all contents within the 'table' tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mdtetMZ_3Dq"
   },
   "outputs": [],
   "source": [
    "all_tables = soup.findAll('table')\n",
    "print(all_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0tvbY3S_3Dw"
   },
   "source": [
    "This returns a collection of tag objects. It seems that most of the information are useless and it's getting hard to hunt for the table. So searched online and found an instruction here: \n",
    "\n",
    "https://adesquared.wordpress.com/2013/06/16/using-python-beautifulsoup-to-scrape-a-wikipedia-table/\n",
    "\n",
    "The class is \"wikitable sortable\"!! Have a try then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c350bOM0_3Dz"
   },
   "outputs": [],
   "source": [
    "right_table = soup.find('table', class_='wikitable sortable')\n",
    "print(right_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbqSZ0i7_3D5"
   },
   "source": [
    "Next we need to extract table header row by find the first 'tr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt3kyOGd_3D7"
   },
   "outputs": [],
   "source": [
    "head_row = right_table.find('tr')\n",
    "print(head_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uj-r3Xhs_3EB"
   },
   "source": [
    "Then we extract header row name via iterate through each row and extract text. \n",
    "\n",
    "The `.findAll` function in Python returns a list containing all the elements, which you can iterate through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLwGVlDi_3ED"
   },
   "outputs": [],
   "source": [
    "header_list = []\n",
    "headers = head_row.findAll('th')\n",
    "for header in headers:\n",
    "    #print header.find(text = True)\n",
    "    header_list.append(header.find(text = True).strip())\n",
    "header_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZNmqW_J_3EK"
   },
   "source": [
    "We can probably iterate through this list and then extract contents. But let's take a simple approach of extracting each column separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNw5o7xQ_3EO"
   },
   "outputs": [],
   "source": [
    "flag = []\n",
    "state = []\n",
    "abbrev = []\n",
    "ISO = []\n",
    "Capital = []\n",
    "Population = []\n",
    "Area = []\n",
    "Seats = []\n",
    "Gov = []\n",
    "Premier = []\n",
    "\n",
    "for row in right_table.findAll(\"tr\"):\n",
    "    cells = row.findAll('td')\n",
    "    if len(cells) > 0 : # and len(cells) < 10:\n",
    "        flag.append(cells[0].find(text=True))\n",
    "        state.append(cells[1].find(text=True).strip())\n",
    "        abbrev.append(cells[2].find(text=True).strip())\n",
    "        ISO.append(cells[3].find(text=True).strip())\n",
    "        Capital.append(cells[4].find(text=True).strip())\n",
    "        Population.append(cells[5].find(text=True).strip())\n",
    "        Area.append(cells[6].find(text=True).strip())\n",
    "        Seats.append(cells[7].find(text=True).strip())\n",
    "        Gov.append(cells[8].find(text=True).strip())\n",
    "        Premier.append(cells[10].find(text=True).strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aB_ywSZe_3EZ"
   },
   "source": [
    "Next we can append all list to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0KLUyg9_3Eb"
   },
   "outputs": [],
   "source": [
    "df_au = pd.DataFrame()\n",
    "df_au[header_list[0]] = flag\n",
    "df_au[header_list[1]] = state\n",
    "df_au[header_list[2]] = abbrev\n",
    "df_au[header_list[3]] = ISO\n",
    "df_au[header_list[4]] = Capital\n",
    "df_au[header_list[5]] = Population\n",
    "df_au[header_list[6]] = Area\n",
    "df_au[header_list[7]] = Seats\n",
    "df_au[header_list[8]] = Gov\n",
    "df_au[header_list[9]] = Premier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAkmBKlD_3E1"
   },
   "source": [
    "Done !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0SVh16b8_3E3"
   },
   "outputs": [],
   "source": [
    "df_au"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "s5P7FONd_3CR",
    "5kjxc5gJ_3E-"
   ],
   "name": "M03E-DataAcquisition-II.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
